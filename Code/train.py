import tensorflow as tf
from data_helpers import read_data
from generator import Generator
from discriminator import Discriminator
import os
import time
import numpy as np

batch_size = 16
gamma = 0.7
eps = 1e-8
Lambda = 0.5
learning_rate = 0.001

with tf.Graph().as_default():
    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
    sess = tf.Session(config=session_conf)
    with sess.as_default():

        X = tf.placeholder(tf.float32, [None, 256, 256, 4], name="X")
        X_slice = X[:, :, :, :3]
        input_image = tf.summary.image("input_image", X_slice, max_outputs=3)
        ground_truth_shadow_masks = tf.placeholder(tf.float32, [batch_size, 256, 256], name="y")
        global_step = tf.Variable(0, name="global_step", trainable=False)
        # Generator
        gen = Generator(X, gamma, batch_size)
        gx = gen.tanh  # shadow mask of size 256 * 256 generated by generator
        generator_image = tf.summary.image("input_image", tf.reshape(gx, [-1, 256, 256, 1]), max_outputs=1)
        t1 = tf.scalar_mul(-gamma, tf.matmul(ground_truth_shadow_masks, tf.log(gx + eps), transpose_a=True))
        t2 = tf.scalar_mul(gamma - 1., tf.matmul((1. - ground_truth_shadow_masks), tf.log(1. - gx + eps), transpose_a=True))

        L_data = -tf.reduce_mean(t1 + t2)

        # Discriminator
        dx_real = Discriminator(X, ground_truth_shadow_masks).sigmoid
        dx_fake = Discriminator(X, gx, reuse=True).sigmoid

        L_cGan = -tf.reduce_mean(tf.log(dx_real + eps) + tf.log(1. - dx_fake + eps))

        with tf.variable_scope("D_loss"):
            d_loss = L_cGan + Lambda * L_data
            d_loss_summary = tf.summary.scalar("d_loss", d_loss)
            tf.summary.scalar("d_loss", d_loss)

        with tf.variable_scope("G_loss"):
            g_loss = -(L_cGan + Lambda * L_data)
            g_loss_summary = tf.summary.scalar("g_loss", g_loss)
            tf.summary.scalar("g_loss", g_loss)

        tvar = tf.trainable_variables()
        dvar = [var for var in tvar if 'discriminator' in var.name]
        gvar = [var for var in tvar if 'generator' in var.name]

        with tf.name_scope('train'):
            d_train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(d_loss, var_list=dvar, global_step=global_step)
            g_train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(g_loss, var_list=gvar, global_step=global_step)
        init = tf.global_variables_initializer()
        sess.run(init)

        timestamp = str(int(time.time()))
        out_dir = os.path.abspath(os.path.join("../", "Models", timestamp))
        checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
        checkpoint_prefix = os.path.join(checkpoint_dir, "model")
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)
        train_summary_dir = os.path.join(out_dir, "summaries", "train")
        merged_summary = tf.summary.merge_all()
        # writer = tf.summary.FileWriter(out_dir + "/summaries")
        # writer.add_graph(sess.graph)
        saver = tf.train.Saver(tf.all_variables())

        for batch, e, num in read_data():
            x, y = zip(*batch)
            x = np.array(x)
            y = np.array(y)
            step, d_loss_value = sess.run([d_train_step, d_loss], feed_dict={X: x, ground_truth_shadow_masks: y})
            step, g_loss_value = sess.run([g_train_step, g_loss], feed_dict={X:x, ground_truth_shadow_masks: y})
            step, g_loss_value = sess.run([g_train_step, g_loss], feed_dict={X:x, ground_truth_shadow_masks: y})
            if num % 10 == 0:
                path = saver.save(sess, checkpoint_prefix, global_step=step)
                print("Saved model checkpoint to {}\n".format(path))
            print("Epoch {:g}".format(e), "Batch Number {:g}".format(num), "D_Loss {:g}".format(d_loss_value), "G_Loss {:g}".format(g_loss_value))

